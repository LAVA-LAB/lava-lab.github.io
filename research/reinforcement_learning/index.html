<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>reinforcement learning | AI-FM</title> <meta name="author" content="AI FM"/> <meta name="description" content="AI-FM (Artificial Intelligence & Formal Methods) conducts research around Learning and Verification in decision-making under uncertainty. "/> <meta name="keywords" content="learning, verification, decision-making under uncertainty, ai"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img//logos/logoShield.svg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ai-fm.github.io/research/reinforcement_learning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">AI-FM</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/research/uncertainty_partial_information">Uncertainty and Partial Information</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/reinforcement_learning">Reinforcement Learning</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/dynamical_systems">Dynamical Systems</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">Resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">Photo Gallery</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">For Students</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/projects/#open">Open Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/#ongoing">Ongoing Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/#complete">Complete Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/writing/">Writing Tips</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <div class="row"> <div class="col-sm-9"> <h2 class="post-title"> <i class="fas fa-robot fa-xs"></i> reinforcement learning </h2> <ul> <li>Safe reinforcement learning</li> <li>Offline reinforcement learning</li> </ul> </div> <div class="col-sm-3"> <img class="card-img" src="/assets/img/6.jpg" style="object-fit: cover; height: 90%" alt="image"> </div> </div> </header> <hr> <ul class="post-list"> <li> <div class="profile float-left" style="width: 60%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://raw.githubusercontent.com/lava-lab/improved_spi/main/assets/teaser.gif?raw=true-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://raw.githubusercontent.com/lava-lab/improved_spi/main/assets/teaser.gif?raw=true-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://raw.githubusercontent.com/lava-lab/improved_spi/main/assets/teaser.gif?raw=true-1400.webp"></source> <img src="https://raw.githubusercontent.com/lava-lab/improved_spi/main/assets/teaser.gif?raw=true" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <h3> Improved SPI </h3> <p>Improved bounds for safe policy improvement</p> <ul class="list-inline "> <li class="list-inline-item text-muted small"> • Offline reinforcement learning </li> <li class="list-inline-item text-muted small"> • Sample efficiency </li> <li class="list-inline-item text-muted small"> • Reliability </li> <li class="list-inline-item text-muted small"> • Safety </li> </ul> </div> <div class="clearfix"> <p>The available data is often limited in offline reinforcement learning. Current methods reliably manage to compute new policies that outperform the data collection policy. However, they might require prohibitive amounts of data. As we cannot collect more data, it is primordial to make the most out of the data available. Therefore, we develop a transformation of the underlying MDP with smaller bounds on the minimum amount of data required for improvement <a class="citation" href="#Wienhoft2023more">(Wienhöft et al., 2023)</a>. This allows offline RL algorithms to return better policies without losing reliability.</p> <h4 id="references">References</h4> <ol class="bibliography"><li><span id="Wienhoft2023more">Wienhöft, P., Suilen, M., Simão, T. D., Dubslaff, C., Baier, C., &amp; Jansen, N. (2023). More for Less: Safe Policy Improvement with Stronger Performance Guarantees. <i>IJCAI</i>, 4406–4415.</span></li></ol> </div> </li> <li> <div class="profile float-right" style="width: 20%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://raw.githubusercontent.com/lava-lab/ATM/main/assets/teaser.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://raw.githubusercontent.com/lava-lab/ATM/main/assets/teaser.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://raw.githubusercontent.com/lava-lab/ATM/main/assets/teaser.gif-1400.webp"></source> <img src="https://raw.githubusercontent.com/lava-lab/ATM/main/assets/teaser.gif" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <h3> Act-Then-Measure </h3> <p>Reinforcement learning for partially observable environments with active measuring</p> <ul class="list-inline "> <li class="list-inline-item text-muted small"> • Learning for planning and scheduling </li> <li class="list-inline-item text-muted small"> • Partially observable and unobservable domains </li> <li class="list-inline-item text-muted small"> • Uncertainty and stochasticity in planning and scheduling </li> </ul> </div> <div class="clearfix"> <p>Ever wondered when you should inspect the engine of your car? Or how often an electricity provider should check their cables to minimize outages and maintenance costs? Or how often should a drone use its battery-draining GPS system to keep an accurate idea of its positions? What connects these problems is one core question: <strong>Is the extra information from a measurement worth its cost?</strong></p> <p>In our recent work, we try to solve such problems quickly by making a distinction between <em>control actions</em> (which affect the environment) and <em>measuring actions</em> (which give us information). For the first, we take into account uncertainty about the current situation but ignore it when predicting the future, which makes our method faster. For the second, we describe a novel method to determine when we can rely on our predictions, and when we should measure to eliminate uncertainty instead.</p> <p>Interested in how it performs? Have a look at our <a href="https://ojs.aaai.org/index.php/ICAPS/article/view/27197" target="_blank" rel="noopener noreferrer">ICAPS paper</a> <a class="citation" href="#Krale2023act">(Krale et al., 2023)</a> to find out!</p> <h4 id="references">References</h4> <ol class="bibliography"><li><span id="Krale2023act">Krale, M., Simão, T. D., &amp; Jansen, N. (2023). Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. <i>ICAPS</i>, 212–220.</span></li></ol> </div> </li> <li> <div class="profile float-left" style="width: 60%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://raw.githubusercontent.com/lava-lab/spi_pomdp/main/assets/teaser.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://raw.githubusercontent.com/lava-lab/spi_pomdp/main/assets/teaser.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://raw.githubusercontent.com/lava-lab/spi_pomdp/main/assets/teaser.gif-1400.webp"></source> <img src="https://raw.githubusercontent.com/lava-lab/spi_pomdp/main/assets/teaser.gif" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <h3> SPI-POMDPs </h3> <p>Reliable offline reinforcement learning (RL) with partial observability</p> <ul class="list-inline "> <li class="list-inline-item text-muted small"> • Offline Reinforcement Learning </li> <li class="list-inline-item text-muted small"> • Partial Observability </li> <li class="list-inline-item text-muted small"> • Reliability </li> <li class="list-inline-item text-muted small"> • Safety </li> </ul> </div> <div class="clearfix"> <p>Limited memory is sufficient for reliable offline reinforcement learning (RL) with partial observability.</p> <p>Safe policy improvement (SPI) aims to reliably improve an agent’s performance in an environment where only historical data is available. Typically, SPI algorithms assume that historical data comes from a fully observable environment. In many real-world applications, however, the environment is only partially observable. Therefore, we investigate how to use SPI algorithms in those settings and show that when the agent has enough memory to infer the environment’s dynamics, it can significantly improve its performance <a class="citation" href="#spi-pomdp">(Simão et al., 2023)</a>.</p> <h4 id="references">References</h4> <ol class="bibliography"><li><span id="spi-pomdp">Simão, T. D., Suilen, M., &amp; Jansen, N. (2023). Safe Policy Improvement for POMDPs via Finite-State Controllers. <i>AAAI</i>, 15109–15117.</span></li></ol> </div> </li> </ul> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 AI FM. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>